{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pymorphy2\n",
    "import requests\n",
    "\n",
    "from copy import copy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import clear_output\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_text import SentencepieceTokenizer\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hope i haven't forgotten anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess texts | can last up to 1-2 days\n",
    "_ = html_main_text(info_dict, transfer_dict, query_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "urls_dict = make_urls_dict()\n",
    "queries = make_queries_list()\n",
    "info_dict = make_info_dict()\n",
    "transfer_dict = ref_dict(urls_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new queries\n",
    "queries = queries_spellchecker(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn launch\n",
    "main_nn_launch(queries, num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to launch classinc model submission\n",
    "main_res = main_launch(queries, info_dict, transfer_dict, bigrams=False, add_s=True)\n",
    "save_submission(main_res, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_urls_dict(): # makes dict url: url_number\n",
    "\n",
    "    urls = open(\"./text-relevance-competition-ir-1-ts-fall-2020/urls.numerate.txt\")\n",
    "    urls = urls.read().split(sep='\\n')[:-1]\n",
    "    \n",
    "    urls_dict = dict()\n",
    "\n",
    "    for url in urls:\n",
    "        s = url.split(sep='\\t')\n",
    "\n",
    "        urls_dict[s[1]] = int(s[0])\\\n",
    "    \n",
    "    return urls_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls_dict = make_urls_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_queries_list(): #list of tuples (query_number, query)\n",
    "    \n",
    "    queries = open(\"./queries_texts2.txt\")\n",
    "    queries = queries.read().split(sep='\\n')[:-1]\n",
    "    \n",
    "    new_queries = []\n",
    "\n",
    "    for q in queries:\n",
    "        \n",
    "        l = q.split(sep='\\t')\n",
    "        new_queries.append([int(l[0]), l[1]])\n",
    "        \n",
    "    return new_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries = make_queries_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_form(q): #lematized text\n",
    "    \n",
    "    n_form = ''\n",
    "    morph = MorphAnalyzer()\n",
    "    \n",
    "    for word in q.split():\n",
    "        n_form += morph.normal_forms(word)[0] + ' '\n",
    "        \n",
    "    return n_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text): #simple\n",
    "    \n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\d', ' ', text)\n",
    "    text = re.sub('[a-z]{20,}|[a-z]*_[a-z]*', ' ', text.lower())\n",
    "\n",
    "    text = re.sub('[ ]+', ' ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossine(query_text, text, encodings, model, num=10): #result by cossine distance for query\n",
    "    \n",
    "    query = query_text.split()[:25]\n",
    "    \n",
    "    query_text = ' '\n",
    "    for q in query:\n",
    "        query_text += q + ' '\n",
    "    \n",
    "    query_embedding = model.signatures['question_encoder'](tf.constant([query_text]))['outputs'][0]\n",
    "    \n",
    "    distances = []\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    \n",
    "    for i in encodings['outputs']:\n",
    "        distances.append(cosine(i, query_embedding))\n",
    "    \n",
    "    search_results = np.argsort(np.asarray(distances))[:num]\n",
    "    proper = []\n",
    "    \n",
    "    for i in search_results:\n",
    "        proper.append(text['info'][i])\n",
    "    \n",
    "    return proper\n",
    "#     return np.asarray(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_index_2(sentences, context, model): # makes embeddings for texts\n",
    "    encodings = model.signatures['response_encoder'](input=tf.constant(sentences),\n",
    "                                                     context=tf.constant(context))\n",
    "    return encodings, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_launch_nn_bm(queries, info_dict, transfer_dict, bigrams=False, add_s=False, num=1): \n",
    "    #main calculation for all queries that combines bm25 and nn for this cossine and bm25 functions should be changed\n",
    "    \n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    main_res = []\n",
    "    n = 0\n",
    "    \n",
    "    texts = [0]\n",
    "    for i in range(1, 5):\n",
    "        with open('./queries_texts/lematized_texts/{}.pickle'.format(i), 'rb') as f:\n",
    "            texts.append(pickle.load(f))\n",
    "    \n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3\" \n",
    "    model_ = hub.load(module_url)\n",
    "    print(\"model ready\")\n",
    "    \n",
    "    for q in queries:\n",
    "        \n",
    "        if q[0]+4 <=399:\n",
    "            texts = texts[1:]\n",
    "            with open('./queries_texts/lematized_texts/{}.pickle'.format(q[0]+4), 'rb') as f:\n",
    "                texts.append(pickle.load(f))\n",
    "        else:\n",
    "            text = texts[0]\n",
    "            texts = texts[1:]\n",
    "            texts.append(text)\n",
    "            \n",
    "        if q[0] < num:\n",
    "            continue\n",
    "        model = copy(model_)\n",
    "        print(\"preprocess ended\")\n",
    "        \n",
    "        text_to_nn = dict()\n",
    "        text_to_nn['title'] = []\n",
    "        for i in range(len(texts[0]['title'])):\n",
    "            t = texts[0]['title'][i].split()[:20]\n",
    "            \n",
    "            text = ' '\n",
    "            for tt in t:\n",
    "                text += tt + ' '\n",
    "\n",
    "            text = re.sub('[^a-z|а-я]', ' ', text)\n",
    "            text = re.sub('[ ]+', ' ', text)\n",
    "            if text == ' ':\n",
    "                text = 'q'\n",
    "                \n",
    "            text_to_nn['title'].append(text)\n",
    "        text_to_nn['info'] = texts[0]['info']\n",
    "        \n",
    "        \n",
    "        query = normal_form(q[1].lower())   \n",
    "            \n",
    "        encodings, model = make_index_2(text_to_nn['title'], text_to_nn['title'], model)\n",
    "        res = cossine(query, text_to_nn, encodings, model, 15)\n",
    "        print(\"model finished\")       \n",
    "\n",
    "        \n",
    "#         new_text = dict()\n",
    "#         new_text['title'] = []\n",
    "#         new_text['info'] = []\n",
    "#         new_text['body'] = []\n",
    "#         new_text['a'] = []\n",
    "#         new_text['meta'] = []\n",
    "#         new_text['h1'] = []\n",
    "#         new_text['h2'] = []\n",
    "#         new_text['h3'] = []\n",
    "#         for i in range(len(texts[0]['info'])):\n",
    "\n",
    "#             if texts[0]['info'][i] in res:\n",
    "#                 new_text['title'].append(texts[0]['title'][i])\n",
    "#                 new_text['body'].append(texts[0]['body'][i])\n",
    "#                 new_text['a'].append(texts[0]['a'][i])\n",
    "#                 new_text['meta'].append(texts[0]['meta'][i])\n",
    "#                 new_text['h1'].append(texts[0]['h1'][i])\n",
    "#                 new_text['h2'].append(texts[0]['h2'][i])\n",
    "#                 new_text['h3'].append(texts[0]['h3'][i])\n",
    "#                 new_text['info'].append(texts[0]['info'][i])\n",
    "                   \n",
    "            \n",
    "        t = dict()\n",
    "#         for tag in new_text:\n",
    "#             t[tag] = []\n",
    "#             t[tag] += new_text[tag]\n",
    "\n",
    "        for tag in texts[0]:\n",
    "            t[tag] = []\n",
    "            for text in texts[1:]:\n",
    "                t[tag] += text[tag]\n",
    "                \n",
    "                \n",
    "        res_bm = calc_res_bm25(query, t, len(texts[0]['info']), bigrams, add_s, w=10)\n",
    "#         res_bm = res_bm/np.max(res_bm)\n",
    "        \n",
    "#         print(res)\n",
    "#         res = (np.array(info_dict[q[0]]))[res]\n",
    "        print(\"bm25 finished\")\n",
    "        \n",
    "#         for i in res:\n",
    "#             main_res.append([q[0], i])\n",
    "        res_final = -1*res_nn + 0.1*res_bm/np.max(res_bm)\n",
    "    \n",
    "        answer = np.argsort(res_final)[::-1]\n",
    "        answer = np.array(texts[0]['info'])[answer]\n",
    "        answer = answer[:10]\n",
    "        res = answer\n",
    "\n",
    "        with open('submission04.txt'.format(num), 'a') as f:   \n",
    "            for i in res:\n",
    "                f.write('{},{}\\n'.format(q[0], i))\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print(q[0], 'downloaded')\n",
    "\n",
    "    return main_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# main_res = main_launch_nn_bm(queries, info_dict, transfer_dict, bigrams=False, add_s=True, num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts[0]['title'][texts[0]['info'].index(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_nn_launch(queries, num=1): #main launch that gives final submission made by USE model\n",
    "    \n",
    "    with open('submission08.txt', 'w') as f:\n",
    "        f.write('QueryId,DocumentId\\n')\n",
    "    \n",
    "    main_res = []\n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3\"\n",
    "    model_ = hub.load(module_url)\n",
    "    print(\"model ready\")\n",
    "    \n",
    "    for q in queries:\n",
    "        if q[0] < num:\n",
    "            continue\n",
    "            \n",
    "        model = copy(model_)\n",
    "        with open('./queries_texts/lematized_texts/{}.pickle'.format(q[0]), 'rb') as f:\n",
    "            text = pickle.load(f)\n",
    "        \n",
    "#         for i in range(len(text['title'])):\n",
    "#             t = text['title'][i].split()[:30]\n",
    "\n",
    "#             new_text = ' '\n",
    "#             for tt in t:\n",
    "#                 new_text += tt + ' '\n",
    "\n",
    "#             text['title'][i] = new_text   \n",
    "            \n",
    "        new_text = dict()\n",
    "        new_text['title'] = []\n",
    "        new_text['info'] = []\n",
    "\n",
    "        for i in range(len(text['info'])):\n",
    "            length = len(text['title'][i].split())\n",
    "            if length > 1 and length < 30:\n",
    "                new_text['title'].append(text['title'][i])\n",
    "                new_text['info'].append(text['info'][i])\n",
    "\n",
    "        print(\"model launch\")\n",
    "        encodings, model = make_index_2(new_text['title'], new_text['title'], model)\n",
    "        res = cossine(normal_form(q[1]), new_text, encodings, model)\n",
    "    \n",
    "        with open('submission08.txt'.format(num), 'a') as f:   \n",
    "            for i in res:\n",
    "                f.write('{},{}\\n'.format(q[0], i))\n",
    "                \n",
    "        clear_output(wait=True)\n",
    "        print(q[0], 'downloaded')\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_nn_launch(queries, num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_info_dict(): #makes dict of queries whith relevant docs list for each\n",
    "    \n",
    "    info = open(\"./text-relevance-competition-ir-1-ts-fall-2020/sample.technosphere.ir1.textrelevance.submission.txt\")\n",
    "    info_list = info.read().split(sep='\\n')[1:-1]\n",
    "    \n",
    "    info_dict = dict()\n",
    "\n",
    "    for info in info_list:\n",
    "\n",
    "        l = info.split(sep=',')\n",
    "        query, doc = int(l[0]), int(l[1])\n",
    "\n",
    "        if query in info_dict.keys():\n",
    "            info_dict[query].append(doc)\n",
    "        else:\n",
    "            info_dict[query] = [doc]\n",
    "            \n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_dict = make_info_dict()\n",
    "# info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_dict(urls_dict): #unifies url numbers with docs numbers\n",
    "    \n",
    "    directories = os.listdir(\"./text-relevance-competition-ir-1-ts-fall-2020/content/content/\")\n",
    "    \n",
    "    texts = []\n",
    "    base_num = 0\n",
    "    doc_num = -1\n",
    "    transfer_dict = dict()\n",
    "    \n",
    "    for directory in directories:\n",
    "\n",
    "        if directory != '.DS_Store':\n",
    "            \n",
    "            files = os.listdir(\"./text-relevance-competition-ir-1-ts-fall-2020/content/content/\" + directory)\n",
    "            files.sort()\n",
    "            base_num += doc_num + 1\n",
    "            \n",
    "            for file in files:\n",
    "                \n",
    "                doc_num = int(re.sub('\\D', '', file))\n",
    "                \n",
    "                name = \"./text-relevance-competition-ir-1-ts-fall-2020/content/content/\" + directory + '/' + file\n",
    "                f = open(name, errors='ignore')\n",
    "                ref = f.readline()[:-1]\n",
    "#                 soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "                \n",
    "#                 t = soup.find('body').text[:1200]\n",
    "#                 ref = re.findall('.*\\n', t)[0][:-1]\n",
    "                \n",
    "#                 print(doc_num + base_num, directory)\n",
    "#                 clear_output(wait=True)\n",
    "                \n",
    "                transfer_dict[doc_num + base_num] = urls_dict[ref]\n",
    "                \n",
    "    return transfer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# transfer_dict = ref_dict(urls_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transfer_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_main_text(info_dict, transfer_dict, query_id): \n",
    "    # extracts and saves lists of lematized texts for each tag for each query\n",
    "    \n",
    "    directories = os.listdir(\"./text-relevance-competition-ir-1-ts-fall-2020/content/content/\")\n",
    "    \n",
    "    texts = []\n",
    "    base_num = 0\n",
    "    doc_num = -1\n",
    "    morph = MorphAnalyzer()\n",
    "    query_texts_dict = dict()\n",
    "    \n",
    "    a_text_list = []\n",
    "    h1_text_list = []\n",
    "    h2_text_list = []\n",
    "    h3_text_list = []\n",
    "    body_text_list = []\n",
    "    title_text_list = []\n",
    "    meta_text_list = []\n",
    "    \n",
    "#     with open('./queries_texts/lematized_texts/{}.pickle'.format(query_id), 'rb') as f: #from 1 to 399\n",
    "#         query_texts_dict = pickle.load(f)\n",
    "        \n",
    "    for directory in directories:\n",
    "\n",
    "        if directory != '.DS_Store':\n",
    "            \n",
    "            files = os.listdir(\"./text-relevance-competition-ir-1-ts-fall-2020/content/content/\" + directory)\n",
    "            files.sort()\n",
    "            base_num += doc_num + 1\n",
    "#             print(directory, base_num)\n",
    "            for file in files:\n",
    "                \n",
    "                doc_num = int(re.sub('\\D', '', file))\n",
    "#                 print()\n",
    "                if transfer_dict[doc_num + base_num] not in info_dict[query_id]:\n",
    "                    continue\n",
    "#                 print(transfer_dict[doc_num + base_num], doc_num, directory)\n",
    "                name = \"./text-relevance-competition-ir-1-ts-fall-2020/content/content/\" + directory + '/' + file\n",
    "                f = open(name, errors='ignore')\n",
    "                f = f.read()\n",
    "                    \n",
    "                soup = BeautifulSoup(f, \"lxml\")\n",
    "                \n",
    "                title = soup.find('title')\n",
    "                h1 = soup.find('h1')\n",
    "                h2 = soup.find('h2')\n",
    "                h3 = soup.find('h3')\n",
    "                body = soup.find('body')\n",
    "                a_list = soup.find_all('a')\n",
    "                meta_list = soup.find_all('meta')\n",
    "                \n",
    "                meta_text = ''\n",
    "                for tag in meta_list[1:]:\n",
    "                    if tag.get('content') and (tag.get('name') in ['description', 'keywords'] or\\\n",
    "                                               tag.get('itemprop') == 'name'):# or i.get('property') == \"og:description\") :\n",
    "                        meta_text += tag.get('content')\n",
    "                        \n",
    "                full_t = ''\n",
    "\n",
    "                for qw in clear_text(title.text if title else '').split():   \n",
    "                    word = morph.normal_forms(qw)[0]\n",
    "                    full_t += word + ' '\n",
    "                    \n",
    "                a_text = ''\n",
    "                for a in a_list:\n",
    "                    a_text += a.text + ' '\n",
    "                    \n",
    "                meta_text_list.append(full_t)\n",
    "                a_text_list.append(clear_text(a_text))\n",
    "                h1_text_list.append(clear_text(h1.text) if h1 else '')\n",
    "                h2_text_list.append(clear_text(h2.text) if h2 else '')\n",
    "                h3_text_list.append(clear_text(h3.text) if h3 else '')\n",
    "                body_text_list.append(clear_text(body.text) if body else '')\n",
    "                title_text_list.append(full_t)\n",
    "                    \n",
    "    query_texts_dict['a'] = a_text_list \n",
    "    query_texts_dict['h1'] = h1_text_list\n",
    "    query_texts_dict['h2'] = h2_text_list\n",
    "    query_texts_dict['h3'] = h3_text_list\n",
    "    query_texts_dict['body'] = body_text_list\n",
    "    query_texts_dict['title'] = title_text_list\n",
    "    query_texts_dict['meta'] = meta_text_list\n",
    "    \n",
    "    with open('./queries_texts/lematized_texts/{}.pickle'.format(query_id), 'wb') as f: #from 1 to 399\n",
    "        pickle.dump(query_texts_dict, f)\n",
    "                \n",
    "    return query_texts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_info(info_dict, transfer_dict, query_id): \n",
    "    #adds information list of doc numbers for each query as i have forgottrn to do it before\n",
    "    \n",
    "    directories = os.listdir(\"./text-relevance-competition-ir-1-ts-fall-2020/content/content/\")\n",
    "    \n",
    "    texts = []\n",
    "    base_num = 0\n",
    "    doc_num = -1\n",
    "    morph = MorphAnalyzer()\n",
    "    query_texts_dict = dict()\n",
    "    \n",
    "    info = []\n",
    "    \n",
    "    for directory in directories:\n",
    "\n",
    "        if directory != '.DS_Store':\n",
    "            \n",
    "            files = os.listdir(\"./text-relevance-competition-ir-1-ts-fall-2020/content/content/\" + directory)\n",
    "            files.sort()\n",
    "            base_num += doc_num + 1\n",
    "#             print(directory, base_num)\n",
    "            for file in files:\n",
    "                \n",
    "                doc_num = int(re.sub('\\D', '', file))\n",
    "#                 print()\n",
    "                if transfer_dict[doc_num + base_num] not in info_dict[query_id]:\n",
    "                    continue\n",
    "#                 print(transfer_dict[doc_num + base_num], doc_num, directory)\n",
    "                info.append(transfer_dict[doc_num + base_num])\n",
    "\n",
    "    with open('./queries_texts/lematized_texts/{}.pickle'.format(query_id), 'rb') as f:\n",
    "        t = pickle.load(f)\n",
    "        \n",
    "    t['info'] = info\n",
    "    \n",
    "    with open('./queries_texts/lematized_texts/{}.pickle'.format(query_id), 'wb') as ff:\n",
    "            pickle.dump(t, ff)\n",
    "\n",
    "    return info               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams_calc(q, texts): #Tf-Idf for bigrams (not used as they haven't bettered result)\n",
    "    \n",
    "    res = 0\n",
    "    \n",
    "    vec = TfidfVectorizer(ngram_range = (2,2))\n",
    "    X = vec.fit_transform(texts)\n",
    "    X = X.toarray()\n",
    "\n",
    "    d = dict(enumerate(vec.get_feature_names()))\n",
    "    bigrams = {v: k for k, v in d.items()}\n",
    "\n",
    "    freqs = np.array(X.sum(axis=0))[0]\n",
    "    bigrams_freqs_sum = freqs.sum()\n",
    "    \n",
    "    sample = q\n",
    "    \n",
    "    if len(sample) > 1:\n",
    "        for i in range(len(sample)-1, 0, -1):\n",
    "            \n",
    "            bigram = sample[i-1] + ' ' + sample[i]\n",
    "            v = np.zeros(X.shape[0])\n",
    "            \n",
    "            if bigram in bigrams:\n",
    "                v = X[:, bigrams[bigram]]\n",
    "                \n",
    "            v = np.where(v != 0, v, bigrams_freqs_sum)\n",
    "#             v = np.log(v)\n",
    "\n",
    "            res += v\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm_25(query_dict, N, lens, w, l): #calculates bm25 for each word in query in vector form\n",
    "    \n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    \n",
    "    res = np.zeros(N)\n",
    "    bm_dict = dict()\n",
    "    \n",
    "    for word in query_dict:\n",
    "        \n",
    "        freqs = query_dict[word]\n",
    "        num = np.where(freqs>0)[0].shape[0]\n",
    "\n",
    "        idf = np.log((N - num + 0.5)/(num + 0.5))\n",
    "        idf = np.where(idf > 0, idf, 0)\n",
    "        \n",
    "        lens = np.asarray(lens)\n",
    "        score = idf * (w*freqs * (k1 + 1))/(w*freqs + k1*(1 - b + b*(lens/np.mean(lens[:l]))))\n",
    "\n",
    "        bm_dict[word] = score\n",
    "        res += score\n",
    "    \n",
    "    return res, bm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_bm25(q, texts, length, w): #bm 25 preparation for texts of exact tag for one query\n",
    "    \n",
    "    morph = MorphAnalyzer()\n",
    "    vect = CountVectorizer()\n",
    "\n",
    "    X = vect.fit_transform(texts)\n",
    "    X = X.toarray()\n",
    "    \n",
    "#     w_num = []\n",
    "#     for i in range(length):\n",
    "#         w_num.append(np.where(X[:, i] > 0)[0].shape[0])\n",
    "    \n",
    "#     w_num = np.asarray(w_num)\n",
    "    \n",
    "    lens = []\n",
    "    for text in texts:\n",
    "        lens.append(len(text))\n",
    "    \n",
    "    query_dict = dict()\n",
    "#     cutted_dict = dict()\n",
    "    \n",
    "    names_dict = dict()\n",
    "    names = vect.get_feature_names()\n",
    "    \n",
    "    for i in range(len(names)):\n",
    "        names_dict[names[i]] = X[:, i]\n",
    "        \n",
    "    for word in q.split():\n",
    "\n",
    "        if len(word) == 1:\n",
    "            continue\n",
    "        \n",
    "        word = morph.normal_forms(word)[0]\n",
    "#         print(word)\n",
    "        \n",
    "        if word in names_dict.keys():\n",
    "            vec = names_dict[word]\n",
    "            query_dict[word] = vec\n",
    "#             cutted_dict[word] = vec[:length]\n",
    "            \n",
    "    res, bm_q = bm_25(query_dict, X.shape[0], lens, w, length)\n",
    "    res = res[:length]\n",
    "#     matrix = matrix[:length, :]\n",
    "    for word in bm_q:\n",
    "        bm_q[word] = bm_q[word][:length]\n",
    "    \n",
    "    return res, bm_q#, (1 - (w_num/length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_res_bm25(q, t, length, bigrams=False, add_s=False, n=10, w=10): #bm 25 general for all tags for exact query\n",
    "\n",
    "\n",
    "#         bad_w = False\n",
    "        \n",
    "#         if add_s == True:\n",
    "# #             print(word)\n",
    "#             synonyms = requests.post('https://synonymonline.ru/assets/json/synonyms.json',\\\n",
    "#                                      data = {\"word\": word}).json()\n",
    "# #             print(synonyms)\n",
    "#             if synonyms['status'] == 'typo':\n",
    "#                 synonyms = synonyms['items'][:2] \n",
    "#                 bad_w = True\n",
    "#             else:\n",
    "#                 synonyms = synonyms['synonyms'][:int(n/2)] if synonyms['status'] != 'notfound' else []\n",
    "#         else:\n",
    "#             synonyms = []\n",
    "            \n",
    "#         for synonym in synonyms:\n",
    "#             if synonym in names_dict.keys():\n",
    "#                 vec = names_dict[synonym]\n",
    "#                 query_dict[synonym] = vec if bad_w == True else vec/n\n",
    "    coef_dict = dict()\n",
    "    coef_dict['a'] = 0.05\n",
    "    coef_dict['title'] = 2\n",
    "    coef_dict['h1'] = 1\n",
    "    coef_dict['h2'] = 1\n",
    "    coef_dict['h3'] = 1\n",
    "    coef_dict['h'] = 1\n",
    "    coef_dict['meta'] = 1\n",
    "    coef_dict['body'] = 0.5\n",
    "    \n",
    "    res = np.zeros(length)\n",
    "    r = np.zeros(length)\n",
    "    \n",
    "    new_t = dict()\n",
    "    new_t = t\n",
    "#     new_t['h'] = []\n",
    "#     for i in range(len(t['info'])):\n",
    "#         new_t['h'].append(t['h1'][i] + ' ' + t['h2'][i] + ' ' + t['h3'][i])\n",
    "#     new_t['meta'] = t['meta']\n",
    "#     new_t['body'] = t['body']\n",
    "#     new_t['title'] = t['title']\n",
    "#     new_t['a'] = t['a']\n",
    "    \n",
    "#     normal_q = normal_form(q)\n",
    "    \n",
    "    for tag in new_t:\n",
    "        if tag != 'info' and tag != 'a':\n",
    "            \n",
    "            texts = new_t[tag]\n",
    "\n",
    "            tag_res, d = tag_bm25(q, texts, length, coef_dict[tag])\n",
    "#                 d = importance(d)\n",
    "#                 r = words_distanses(q, new_t[tag][:length], w, d)\n",
    "#                 print(np.asarray(t['info'])[np.argsort(np.asarray(r))[::-1]])\n",
    "                \n",
    "#             tag_res = coef_dict[tag] * bm\n",
    "            res += tag_res\n",
    "#             print(tag)\n",
    "#             if tag in ['body', 'a']:\n",
    "#                 m, s = position_stats(q, new_t[tag])\n",
    "#                 print(tag, np.median(tag_res), np.median(m), np.median(s))\n",
    "#                 print(tag, np.mean(tag_res), np.mean(m), np.mean(s))\n",
    "#                 print(m, s)\n",
    "#                 if tag == 'body':\n",
    "# #                     res = res - 0.3*m - 1*s\n",
    "#                     res = res - 3*m - 1*s\n",
    "#                 else:\n",
    "#                     res = res - 4*m - 1*s\n",
    "#                     res = res - 0.1*m - 1*s\n",
    "            \n",
    "#             res += tag_res * (1 + 3*s)/(1 + 2*m)\n",
    "#             res += tag_res * (1 + 5*s)*(1+5*m)\n",
    "    \n",
    "          \n",
    "    \n",
    "#     if bigrams == True:\n",
    "#         res += bigrams_calc(q, t['a'])\n",
    "\n",
    "#     for val in query_dict.values():\n",
    "#         res += np.log(val)\n",
    "        \n",
    "#     print(res)\n",
    "#     res = res[:length]\n",
    "#     print(res, n)\n",
    "#     res += 0.1*n\n",
    "#     res += r\n",
    "\n",
    "    answer = np.argsort(res)[::-1]\n",
    "    answer = np.array(t['info'])[answer]\n",
    "    answer = answer[:w]\n",
    "    \n",
    "    return answer\n",
    "#     return np.asarray(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# main_res = main_launch(queries, info_dict, transfer_dict, bigrams=False, add_s=True, num=395)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_points(d, window,, length, importances, num): \n",
    "    #trying to choose formula to use collacations statistics (not used because it haven't bettered score)\n",
    "    \n",
    "# def calc_points(d, w, length):\n",
    "    \n",
    "    points = 0\n",
    "    \n",
    "    for bigram in d:\n",
    "        stats = d[bigram] \n",
    "#         score = (stats[0]/(len(d))) * (((w+1) - stats[1])/w / stats[2]) if stats[2] else (score / 5)\n",
    "#         score = stats[0]\n",
    "#         score = stats[0]*stats[1]/(length * len(d))\n",
    "        score = stats[0]/(len(d)) * (((window+1) - stats[1])/window)\n",
    "        score = score / stats[2] if stats[2] else score / 5\n",
    "#         score = (stats[0] * stats[1] / len(d))*(stats[2] if stats[2] > 0 else 1)\n",
    "        points += score * importances[bigram][num]\n",
    "        \n",
    "        \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stats(d): #returns statistics for collacations (not used because it haven't bettered score)\n",
    "    \n",
    "    res = dict()\n",
    "    \n",
    "    for bigram in d:\n",
    "        l = d[bigram]\n",
    "        l = np.asarray(l)\n",
    "        res[bigram] = (l.shape[0], np.mean(l), np.std(l)) #mean and std of distances between collacation word pair\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_distanses(q, texts, w, d): \n",
    "    #searches collacations in text and returns score using their stats (not used because it haven't bettered score)\n",
    "    \n",
    "    main_res = []\n",
    "    q = normal_form(q)\n",
    "    q_list = q.split()\n",
    "    q = [q]\n",
    "    \n",
    "#     importance_dict = dict()\n",
    "    importances = dict()\n",
    "#     vec = TfidfVectorizer()\n",
    "#     X = vec.fit_transform(texts)\n",
    "#     X = X.toarray()\n",
    "#     n = vec.get_feature_names()\n",
    "    \n",
    "#     for word in q_list:\n",
    "#         if len(word) > 2:\n",
    "# #             importance_dict[word] = tag_bm25(word, texts)\n",
    "#             if word in n:\n",
    "#                 importance_dict[word] = X[:, n.index(word)]\n",
    "#             else:\n",
    "#                 importance_dict[word] = np.zeros(X.shape[0])\n",
    "\n",
    "    \n",
    "    names = []\n",
    "    for i in range(len(q_list)-1):\n",
    "        for j in range(i+1, len(q_list)):\n",
    "            if len(q_list[i]) > 2 and len(q_list[j]) > 2 and q_list[j] in d and q_list[i] in d:\n",
    "                bigram = q_list[i] + ' ' + q_list[j]\n",
    "                names.append(bigram)\n",
    "                importances[bigram] = d[q_list[i]] + d[q_list[j]]\n",
    "#     print(importances)\n",
    "    for text_num, text in enumerate(texts):\n",
    "        \n",
    "        text = text.split()\n",
    "        res = dict()\n",
    "\n",
    "        for i in range(0, len(text) - w + 1):\n",
    "            window = text[i:i + w]\n",
    "#             print(window)\n",
    "            word = window[0]\n",
    "            for j in range(1, len(window)):\n",
    "                \n",
    "                bigram = word + ' ' + window[j]\n",
    "                \n",
    "                if bigram in names:\n",
    "                    if bigram in res:\n",
    "                        res[bigram].append(j)\n",
    "                    else:\n",
    "                        res[bigram] = [j]\n",
    "                       \n",
    "        res = calc_stats(res)         \n",
    "        main_res.append(calc_points(res, w, len(text), importances, text_num))\n",
    "#         main_res.append(calc_points(res, w, len(text)))\n",
    "        \n",
    "#     return np.asarray(main_res) * 100\n",
    "    return np.log(np.asarray(main_res) * 1000 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recalc_stats(res): #return proccessed stats for next function (not used because it haven't bettered score)\n",
    "    \n",
    "    main_means = []\n",
    "    main_stds = []\n",
    "    \n",
    "    for i in range(len(res)):\n",
    "        \n",
    "        means = []\n",
    "        stds = []\n",
    "        bad_count = 0\n",
    "        \n",
    "        for word in res[i]:\n",
    "            \n",
    "            stats = res[i][word]\n",
    "            \n",
    "            if stats != -1:\n",
    "                means.append(res[i][word][0])\n",
    "                stds.append(res[i][word][1])\n",
    "            else:\n",
    "                bad_count += 1\n",
    "                stds.append(0)\n",
    "                \n",
    "        means = np.asarray(means)\n",
    "        stds = np.asarray(stds)\n",
    "#         print(means.shape, np.std(means), bad_count, len(res[i]))\n",
    "        \n",
    "        main_means.append((np.std(means) if means.shape[0] != 0 else 0) + 0.5*bad_count/len(res[i]))\n",
    "        \n",
    "#         main_means.append(np.std(means) if means.shape[0] != 0 else 0)\n",
    "        main_stds.append(np.sum(stds)*(1 - 0.5*bad_count/len(res[i])))\n",
    "#     print(main_means)\n",
    "    return np.asarray(main_means), np.asarray(main_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_stats(q, texts): \n",
    "    #calculates statistics for words from query (not used because it haven't bettered score)\n",
    "    \n",
    "    main_res = []\n",
    "    q = q.split()\n",
    "    \n",
    "    for text in texts:\n",
    "        \n",
    "        text = text.split()\n",
    "        res = dict()\n",
    "\n",
    "        for word in q:\n",
    "            if len(word) > 1:\n",
    "                res[word] = []\n",
    "\n",
    "        for word_num in range(len(text)):\n",
    "            if text[word_num] in q and len(text[word_num]) > 1:\n",
    "                \n",
    "                res[text[word_num]].append(word_num)\n",
    "\n",
    "        for i in res:\n",
    "            if res[i] != []:\n",
    "                for j in range(len(res[i])):\n",
    "                    res[i][j] /= len(text)\n",
    "\n",
    "                res_ar = np.asarray(res[i])\n",
    "                res[i] = (np.mean(res_ar), np.std(res_ar)) \n",
    "            else:\n",
    "                res[i] = -1\n",
    "#         print(res[2])\n",
    "        main_res.append(res)\n",
    "#     print(main_res)\n",
    "#     return main_res\n",
    "    return recalc_stats(main_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematize_a(query_id): #lematizes tag \"a\" as i have forgotten to do it in main loop\n",
    "    \n",
    "    morph = MorphAnalyzer()\n",
    "    with open('./queries_texts/lematized_texts/{}.pickle'.format(query_id), 'rb') as f:\n",
    "        t = pickle.load(f)\n",
    "    \n",
    "    new_list = []\n",
    "    a_list = t['a']\n",
    "    \n",
    "    for a in a_list:\n",
    "        full_a = ''\n",
    "    \n",
    "        for qw in a.split():   \n",
    "            word = morph.normal_forms(qw)[0]\n",
    "            full_a += word + ' '\n",
    "        \n",
    "        new_list.append(full_a)\n",
    "        \n",
    "    t['a'] = new_list\n",
    "    \n",
    "#     return t\n",
    "    with open('./queries_texts/lematized_texts/{}.pickle'.format(query_id), 'wb') as ff:\n",
    "            pickle.dump(t, ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# idf_dict, freqs_dict, boundaries = IDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDF(): \n",
    "    #calculates idf dict for all queries to fasten submission calculation (not used as its too slow to calc it)\n",
    "    texts = []\n",
    "    boundaries = dict()\n",
    "    boundaries[0] = (0, 0)\n",
    "    \n",
    "    for i in range(1, 400):\n",
    "        with open('./queries_texts/lematized_texts/{}.pickle'.format(i), 'rb') as f:\n",
    "            t = pickle.load(f)\n",
    "            length = len(t['body'])\n",
    "            texts += t['body']\n",
    "            clear_output(wait=True)\n",
    "            print(i)\n",
    "            boundaries[i] = (boundaries[i-1][1], boundaries[i-1][1] + length)\n",
    "    \n",
    "#     t = dict()\n",
    "    print(len(texts))\n",
    "#     print()\n",
    "#     for tag in texts[0]:\n",
    "#         t[tag] = []\n",
    "#     t['body'] = []\n",
    "#     for text in texts:\n",
    "#         t['body'] += text['bo\n",
    "            \n",
    "    vect = CountVectorizer(max_features=200000)\n",
    "    X = vect.fit_transform(texts)\n",
    "    \n",
    "    idf_dict = dict()\n",
    "    freqs_dict = dict()\n",
    "    names = vect.get_feature_names()\n",
    "    print(len(names))\n",
    "    for i in range(len(names)):\n",
    "#         idf_dict[names[i]]\n",
    "        freqs = X[:, i].toarray()\n",
    "#         print(freqs)\n",
    "        freqs_dict[names[i]] = freqs\n",
    "#         freqs = query_dict[word]\n",
    "        num = np.where(freqs>0)[0].shape[0]\n",
    "\n",
    "        idf = np.log((freqs.shape[0] - num + 0.5)/(num + 0.5))\n",
    "        idf = np.where(idf > 0, idf, 0)\n",
    "        \n",
    "        idf_dict[names[i]] = idf\n",
    "        if i % 2000 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(i)\n",
    "        \n",
    "    return idf_dict, freqs_dict, boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance(d): #returns imporatances from bm25 for each word \n",
    "    \n",
    "#     print(list(d.values)[0])\n",
    "    m = np.zeros((len(list(d.values())[0]), len(d)))\n",
    "    \n",
    "    for i, word in enumerate(d):\n",
    "        m[:, i] = d[word]\n",
    "    \n",
    "    m = np.sum(m, axis=0)\n",
    "    res = np.argsort(m)[::-1][:5]\n",
    "    \n",
    "    main_words = dict()\n",
    "    for i, word in enumerate(d):\n",
    "        if i in res:\n",
    "            main_words[word] = d[word]\n",
    "            \n",
    "    return main_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# texts = []\n",
    "# for i in range(1, 6):\n",
    "#     with open('./queries_texts/lematized_texts/{}.pickle'.format(i), 'rb') as f:\n",
    "#         texts.append(pickle.load(f))\n",
    "\n",
    "\n",
    "#     t = dict()\n",
    "#     for tag in texts[0]:\n",
    "#         t[tag] = []\n",
    "#         for text in texts:\n",
    "#             t[tag] += text[tag]\n",
    "            \n",
    "# calc_res_bm25(q, t, len(texts[0]['info']), True, False, w=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t['title'][t['info'].index(57)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(query): #expand query with synonyms (not used)\n",
    "    \n",
    "    morph = MorphAnalyzer()\n",
    "#     for q_num in range(len(queries)):\n",
    "\n",
    "    full_q = ''\n",
    "    for qw in query.split():   \n",
    "        for word in morph.parse(qw)[0].lexeme:\n",
    "            full_q += word.word + ' '\n",
    "\n",
    "    query = full_q\n",
    "        \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_launch(queries, info_dict, transfer_dict, bigrams=False, add_s=False): #main calculation for all queries\n",
    "    \n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    main_res = []\n",
    "    n = 0\n",
    "    \n",
    "    texts = [0]\n",
    "    for i in range(1, 5):\n",
    "        with open('./queries_texts/lematized_texts/{}.pickle'.format(i), 'rb') as f:\n",
    "            texts.append(pickle.load(f))\n",
    "\n",
    "\n",
    "    for q in queries:\n",
    "        \n",
    "        if q[0]+4 <=399:\n",
    "            texts = texts[1:]\n",
    "            with open('./queries_texts/lematized_texts/{}.pickle'.format(q[0]+4), 'rb') as f:\n",
    "                texts.append(pickle.load(f))\n",
    "        else:\n",
    "            text = texts[0]\n",
    "            texts = texts[1:]\n",
    "            texts.append(text)\n",
    "            \n",
    "        t = dict()\n",
    "        for tag in texts[0]:\n",
    "            t[tag] = []\n",
    "            for text in texts:\n",
    "                t[tag] += text[tag]\n",
    "                \n",
    "        res = calc_res_bm25(q[1].lower(), t, len(texts[0]['info']), bigrams, add_s)\n",
    "#         print(res)\n",
    "#         res = (np.array(info_dict[q[0]]))[res]\n",
    "        \n",
    "        for i in res:\n",
    "            main_res.append([q[0], i])\n",
    "            \n",
    "        clear_output(wait=True)\n",
    "        n+=1\n",
    "        print(n, 'downloaded')\n",
    "\n",
    "    return main_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to launch classinc model submission\n",
    "# %%time \n",
    "# main_res = main_launch(queries, info_dict, transfer_dict, bigrams=False, add_s=True)\n",
    "# print(len(main_res))\n",
    "# save_submission(main_res, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(main_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save_submission(main_res, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differ(26, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemitize(): #lematization of all tags\n",
    "    \n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    \n",
    "    for i in range(192, 400):\n",
    "        with open('./queries_texts/{}.pickle'.format(i), 'rb') as f:\n",
    "            t = pickle.load(f)\n",
    "            for tag in t:\n",
    "                if tag =='a':\n",
    "                    continue\n",
    "                l = t[tag]\n",
    "                \n",
    "                for j in range(len(l)):\n",
    "                    q = l[j]\n",
    "                    \n",
    "                    full_q = ''\n",
    "\n",
    "                    for qw in q.split():   \n",
    "                        word = morph.normal_forms(qw)[0]\n",
    "                        full_q += word + ' '\n",
    "\n",
    "                    t[tag][j] = full_q\n",
    "            \n",
    "            with open('./queries_texts/lematized_texts/{}.pickle'.format(i), 'wb') as ff:\n",
    "                pickle.dump(t, ff)\n",
    "                \n",
    "            clear_output(wait=True)\n",
    "            print(i, 'downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_submission(main_res, num=1): # saving submission\n",
    "    \n",
    "    with open('submission{}.txt'.format(num), 'w') as f:\n",
    "        f.write('QueryId,DocumentId\\n')\n",
    "        for i in main_res:\n",
    "            f.write('{},{}\\n'.format(i[0], i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differ(n1=22, n2=23): #differences between two submissions\n",
    "    \n",
    "    with open('submission{}.txt'.format(n1), 'r') as f1:\n",
    "        ans1 = f1.read().splitlines()\n",
    "    with open('submission{}.txt'.format(n2), 'r') as f2:\n",
    "        ans2 = f2.read().splitlines()\n",
    "\n",
    "    k = 0\n",
    "    \n",
    "    for i in range(len(ans1)):\n",
    "        if ans1[i] != ans2[i]:\n",
    "            k += 1\n",
    "            print(ans1[i], ans2[i])\n",
    "        \n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'как pfvtybnm gjgkfdjr d eybnfpt c jrjdjq gjldjlrjq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def correct(q): #bad attempt to correct query using api\n",
    "    n = ''\n",
    "    for w in q.split():\n",
    "        synonyms = requests.post('https://synonymonline.ru/assets/json/synonyms.json', data = {\"word\":w}).json()\n",
    "        if synonyms['status'] == 'typo':\n",
    "            synonyms = synonyms['items'][:1]\n",
    "            n += synonyms[0] + ' '\n",
    "        else:\n",
    "            n += synonyms['word'] + ' '\n",
    "#     else:\n",
    "#         n += w + ' '\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'как pfvtybnm gjgkfdjr d eybnfpt c jrjdjq gjldjlrjq'\n",
    "q = 'что входитв стоимиость авиабилетов'\n",
    "q = 'как лутше замариновать шашлык'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differ_queries(): #doesn't work\n",
    "    \n",
    "    with open('./queries_texts2', 'r') as f1:\n",
    "        ans1 = f1.read().splitlines()\n",
    "    with open('queries_numerate_new.txt', 'r') as f2:\n",
    "        ans2 = f2.read().splitlines()\n",
    "\n",
    "    k = 0\n",
    "    \n",
    "    for i in range(len(ans1)):\n",
    "        \n",
    "        ans1[i] = re.sub('{[ ]+}', ' ', ans1[i]).split('\\t')[1:]\n",
    "        ans2[i] = re.sub('{[ ]+}', ' ', ans2[i]).split('\\t')[1:]\n",
    "        print(ans1[i], ans2[i])\n",
    "        ans1[i] = ans1[i][0][:-1]\n",
    "        ans2[i] = ans2[i][0]\n",
    "        if ans1[i] != ans2[i]:\n",
    "            k += 1\n",
    "#             print(k, ' | ', ans1[i],' | ', ans2[i])\n",
    "        \n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new_q = queries_spellchecker(queries) #query correction loop (than i added some hand corrections, just a little)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queries_spellchecker(queries): #main correction function\n",
    "    \n",
    "    new_q = [\n",
    "    for q in queries:\n",
    "        \n",
    "        with open('./queries_texts/{}.pickle'.format(q[0]), 'rb') as f:\n",
    "            t = pickle.load(f)\n",
    "        body = t['body']\n",
    "        \n",
    "        texts = ''\n",
    "        for text in body:\n",
    "            texts += text + ' '\n",
    "            \n",
    "        with open('text.txt', 'w') as f:\n",
    "            f.write(texts)\n",
    "            \n",
    "        s = Spellcheker(\"./text.txt\")\n",
    "        correct = s.correct(clear_text(q[1]))\n",
    "        print(q[0], ' | ', q[1], ' | ', correct)\n",
    "        new_q.append([q[0], correct])\n",
    "        \n",
    "    with open('./queries_texts2.txt', 'w') as f2:\n",
    "        for i in range(len(new_q)):\n",
    "            f2.write('{}\\t{}\\n'.format(new_q[i][0], new_q[i][1]))\n",
    "    return new_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_text_txt(num): \n",
    "    \n",
    "#     with open('./queries_texts/{}.pickle'.format(num), 'rb') as f:\n",
    "#         t = pickle.load(f)\n",
    "    \n",
    "#     body = t['body']\n",
    "#     h1 = t['h1']\n",
    "    \n",
    "#     texts = ''\n",
    "#     for text in body:\n",
    "#         texts += text + ' '\n",
    "#     for text in h1:\n",
    "#         texts += text + ' '\n",
    "        \n",
    "#     with open('text.txt', 'w') as f:\n",
    "#         f.write(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language(self, word): #changes if it's nessesary\n",
    "\n",
    "    eng = \"qwertyuiop[]asdfghjkl;'zxcvbnm,\"\n",
    "    rus = \"йцукенгшщзхъфывапролджэячсмитьб\"\n",
    "    rus_transfer = dict(zip(rus, eng))\n",
    "    eng_transfer = dict(zip(eng, rus))\n",
    "\n",
    "    keyboard, keyboard_t = (eng, eng_transfer) if word[0] in eng else (rus, rus_transfer)\n",
    "\n",
    "    for i in range(len(word)):\n",
    "        if word[i] not in keyboard:\n",
    "            return word\n",
    "\n",
    "    for i in range(len(word)):\n",
    "        word = word[:i] + keyboard_t[word[i]] + word[i+1:]\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spellcheker: #class Spellcheker used to correct queries (not from spellchecker homework)\n",
    "    \n",
    "    def __init__(self, corpus_path):\n",
    "        \n",
    "        self.sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "        self.sym_spell.create_dictionary(corpus_path)\n",
    "        self.morph = MorphAnalyzer()\n",
    "        self.freqs_sum = sum(self.sym_spell.words.values())\n",
    "    \n",
    "    \n",
    "    def language(self, word):\n",
    "\n",
    "        eng = \"qwertyuiop[]asdfghjkl;'zxcvbnm,\"\n",
    "        rus = \"йцукенгшщзхъфывапролджэячсмитьб\"\n",
    "        rus_transfer = dict(zip(rus, eng))\n",
    "        eng_transfer = dict(zip(eng, rus))\n",
    "\n",
    "        keyboard, keyboard_t = (eng, eng_transfer) if word[0] in eng else (rus, rus_transfer)\n",
    "\n",
    "        for i in range(len(word)):\n",
    "            if word[i] not in keyboard:\n",
    "                return word\n",
    "\n",
    "        for i in range(len(word)):\n",
    "            word = word[:i] + keyboard_t[word[i]] + word[i+1:]\n",
    "\n",
    "        return word\n",
    "    \n",
    "    \n",
    "    def translit(self, query):\n",
    "        \n",
    "        changed = ' '\n",
    "        \n",
    "        for word in query.split():\n",
    "            changed += self.language(word) + ' '\n",
    "            \n",
    "        return changed\n",
    "    \n",
    "    \n",
    "    def correct_word(self, word, c=1):\n",
    "        result = self.sym_spell.word_segmentation(word)\n",
    "#         print(\"{}, {}, {}\".format(result.corrected_string, result.distance_sum,\n",
    "#                           result.log_prob_sum))\n",
    "#         s0 = ((np.log(result.distance_sum/10)) if result.distance_sum != 0 else 0) + result.log_prob_sum\n",
    "#         s0 += np.log(c)\n",
    "        \n",
    "        res = ''\n",
    "        score = 0\n",
    "        \n",
    "        main_scores = []\n",
    "        main_strings = []\n",
    "        \n",
    "#         u = self.sym_spell.lookup(word, Verbosity.ALL,\n",
    "#                                  max_edit_distance=2, include_unknown=True)\n",
    "#         for uu in u:\n",
    "#             print(str(uu))\n",
    "        \n",
    "#         print(self.freqs_sum)\n",
    "        for w in result.corrected_string.split():\n",
    "            s = self.sym_spell.lookup(w, Verbosity.ALL,\n",
    "                                 max_edit_distance=2, include_unknown=True)\n",
    "            scores = []\n",
    "            strings = []\n",
    "            for i in range(len(s)):\n",
    "                info = str(s[i]).split(sep=', ')\n",
    "                info[1], info[2] = int(info[1]), int(info[2])\n",
    "                scores.append((np.log((10 - info[1])/100) if info[1] != 0 else 0) + 0.5*(np.log((info[2]/self.freqs_sum)) if info[2] != 0 else 0))\n",
    "#                 print('log_sum2 = ', np.log((10 - info[1])/100) if info[1] != 0 else 0,  np.log(info[2]/self.freqs_sum), info[2], info[0])\n",
    "#                 print(info[0])\n",
    "#                 scores.append(info[1])\n",
    "                strings.append(info[0])\n",
    "                \n",
    "            scores = np.asarray(scores)\n",
    "            best = np.argmax(scores)\n",
    "            \n",
    "            res += strings[best] + ' '\n",
    "            score += scores[best]\n",
    "            \n",
    "        s = self.sym_spell.lookup(word, Verbosity.ALL,\n",
    "                             max_edit_distance=2, include_unknown=True)\n",
    "        score += np.log(c)\n",
    "#         print(score, s0)\n",
    "        return res, score\n",
    "        return result.corrected_string, s0\n",
    "        return (res, score) if score > s0 else (result.corrected_string, s0)\n",
    "    \n",
    "    \n",
    "    def correct(self, query):\n",
    "        \n",
    "#         normal_q = ''\n",
    "#         for word in query.split():\n",
    "#             normal_q += self.morph.normal_forms(word)[0] + ' '\n",
    "#         query = normal_q\n",
    "#         query = normal_form(query)\n",
    "#         print(query)\n",
    "\n",
    "        res1 = ['', 0]\n",
    "        res2 = ['', 0]\n",
    "        \n",
    "        for word in query.split():\n",
    "            \n",
    "            res = self.correct_word(word)\n",
    "#             print(word, res)\n",
    "            res1[0] += res[0] + ' '\n",
    "            res1[1] += res[1]\n",
    "            \n",
    "            res = self.correct_word(self.language(word), 0.1)\n",
    "#             print(res)\n",
    "            res2[0] += res[0] + ' '\n",
    "            res2[1] += res[1]\n",
    "        \n",
    "#         res1 = re.sub('[ ]+', ' ', res1)\n",
    "#         res2 = re.sub('[ ]+', ' ', res2)\n",
    "        \n",
    "        if res1[1] >= res2[1]:\n",
    "            return re.sub('[ ]+', ' ', res1[0])\n",
    "        else:\n",
    "            return re.sub('[ ]+', ' ', res2[0])\n",
    "#         suggestions1 = self.sym_spell.lookup_compound(query, max_edit_distance=2)\n",
    "# #                                                       transfer_casing=True)\n",
    "        \n",
    "#         suggestions2 = self.sym_spell.lookup_compound(self.translit(query), max_edit_distance=2)\n",
    "# #                                                       transfer_casing=True)\n",
    "        \n",
    "        \n",
    "#         suggestions1 = str(suggestions1[0]).split(sep=',') if suggestions1 else ['', '99']\n",
    "#         suggestions2 = str(suggestions2[0]).split(sep=',') if suggestions2 else ['', '100']\n",
    "# #         print(suggestions1, suggestions2)\n",
    "#         best = np.argmin((int(suggestions1[1]), int(suggestions2[1]))) + 1\n",
    "        \n",
    "#         return normal_form(suggestions1[0] if best == 1 else suggestions2[0])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# a = Spellcheker(\"./text.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.correct('каквернуть налог в аэропорту парижа')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
